"""
Comprehensive Test Runner for Certify Studio

This test runner executes all test suites in the correct order and provides
detailed reporting of test results with beautiful colored output.
"""

import os
import sys
import time
import json
import asyncio
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Tuple
import subprocess
import pytest
from colorama import init, Fore, Style, Back

# Initialize colorama for Windows
init()

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


class TestRunner:
    """Comprehensive test runner with beautiful reporting"""
    
    def __init__(self):
        self.start_time = None
        self.test_results = {
            "timestamp": datetime.now().isoformat(),
            "suites": [],
            "summary": {}
        }
        self.output_dir = Path("tests/outputs")
        self.output_dir.mkdir(exist_ok=True)
    
    def print_header(self):
        """Print beautiful header"""
        print(f"\n{Fore.CYAN}{'='*80}{Style.RESET_ALL}")
        print(f"{Fore.CYAN}â•‘{Style.RESET_ALL} {Fore.WHITE}{Style.BRIGHT}CERTIFY STUDIO - COMPREHENSIVE TEST SUITE{Style.RESET_ALL} {Fore.CYAN}â•‘{Style.RESET_ALL}".center(90))
        print(f"{Fore.CYAN}{'='*80}{Style.RESET_ALL}")
        print(f"{Fore.YELLOW}ðŸš€ AI-Powered Certification Content Generation Platform{Style.RESET_ALL}".center(80))
        print(f"{Fore.CYAN}{'='*80}{Style.RESET_ALL}\n")
    
    def print_section(self, title: str, icon: str = "ðŸ“‹"):
        """Print section header"""
        print(f"\n{Fore.GREEN}{icon} {title}{Style.RESET_ALL}")
        print(f"{Fore.GREEN}{'-' * (len(title) + 4)}{Style.RESET_ALL}")
    
    def run_unit_tests(self) -> Tuple[int, int]:
        """Run unit tests"""
        self.print_section("UNIT TESTS", "ðŸ§ª")
        
        test_files = [
            "tests/unit/test_agents.py",
            "tests/unit/test_models.py",
            "tests/unit/test_services.py",
            "tests/unit/test_utils.py"
        ]
        
        passed = 0
        failed = 0
        
        for test_file in test_files:
            if Path(test_file).exists():
                print(f"{Fore.BLUE}Running {test_file}...{Style.RESET_ALL}")
                result = pytest.main([test_file, "-v", "--tb=short", "-q"])
                
                if result == 0:
                    passed += 1
                    print(f"{Fore.GREEN}âœ“ {test_file} passed{Style.RESET_ALL}")
                else:
                    failed += 1
                    print(f"{Fore.RED}âœ— {test_file} failed{Style.RESET_ALL}")
            else:
                print(f"{Fore.YELLOW}âš  {test_file} not found, creating...{Style.RESET_ALL}")
                self._create_unit_test_template(test_file)
        
        return passed, failed
    
    def run_integration_tests(self) -> Tuple[int, int]:
        """Run integration tests"""
        self.print_section("INTEGRATION TESTS", "ðŸ”§")
        
        test_files = [
            "tests/integration/test_api_integration.py",
            "tests/integration/test_agent_collaboration.py",
            "tests/integration/test_database_integration.py",
            "tests/integration/test_knowledge_graph.py"
        ]
        
        passed = 0
        failed = 0
        
        for test_file in test_files:
            if Path(test_file).exists():
                print(f"{Fore.BLUE}Running {test_file}...{Style.RESET_ALL}")
                result = pytest.main([test_file, "-v", "--tb=short", "-q"])
                
                if result == 0:
                    passed += 1
                    print(f"{Fore.GREEN}âœ“ {test_file} passed{Style.RESET_ALL}")
                else:
                    failed += 1
                    print(f"{Fore.RED}âœ— {test_file} failed{Style.RESET_ALL}")
            else:
                print(f"{Fore.YELLOW}âš  {test_file} not found, creating...{Style.RESET_ALL}")
                self._create_integration_test_template(test_file)
        
        return passed, failed
    
    def run_e2e_tests(self) -> Tuple[int, int]:
        """Run end-to-end tests"""
        self.print_section("END-TO-END TESTS", "ðŸŽ¯")
        
        # Special handling for AWS test
        print(f"{Fore.MAGENTA}ðŸŽ¯ Running AWS AI Practitioner Complete Workflow Test...{Style.RESET_ALL}")
        
        test_file = "tests/e2e/test_aws_ai_practitioner_complete.py"
        result = pytest.main([test_file, "-v", "-s", "--tb=short"])
        
        if result == 0:
            print(f"{Fore.GREEN}âœ“ AWS AI Practitioner E2E test passed!{Style.RESET_ALL}")
            return 1, 0
        else:
            print(f"{Fore.RED}âœ— AWS AI Practitioner E2E test failed{Style.RESET_ALL}")
            return 0, 1
    
    def run_performance_tests(self) -> Tuple[int, int]:
        """Run performance tests"""
        self.print_section("PERFORMANCE TESTS", "âš¡")
        
        print(f"{Fore.BLUE}Running performance benchmarks...{Style.RESET_ALL}")
        
        # Create performance test if it doesn't exist
        perf_test_file = "tests/performance/test_performance.py"
        if not Path(perf_test_file).exists():
            self._create_performance_test()
        
        result = pytest.main([perf_test_file, "-v", "--tb=short"])
        
        if result == 0:
            print(f"{Fore.GREEN}âœ“ Performance tests passed{Style.RESET_ALL}")
            return 1, 0
        else:
            print(f"{Fore.RED}âœ— Performance tests failed{Style.RESET_ALL}")
            return 0, 1
    
    def check_code_coverage(self):
        """Check code coverage"""
        self.print_section("CODE COVERAGE", "ðŸ“Š")
        
        try:
            # Run coverage
            subprocess.run([
                sys.executable, "-m", "coverage", "run", 
                "-m", "pytest", "tests/", "-q"
            ], capture_output=True)
            
            # Generate report
            result = subprocess.run([
                sys.executable, "-m", "coverage", "report"
            ], capture_output=True, text=True)
            
            if result.returncode == 0:
                print(result.stdout)
                
                # Extract coverage percentage
                lines = result.stdout.split('\n')
                for line in lines:
                    if "TOTAL" in line:
                        parts = line.split()
                        coverage = parts[-1]
                        coverage_value = int(coverage.rstrip('%'))
                        
                        if coverage_value >= 80:
                            print(f"{Fore.GREEN}âœ“ Code coverage: {coverage}{Style.RESET_ALL}")
                        elif coverage_value >= 60:
                            print(f"{Fore.YELLOW}âš  Code coverage: {coverage}{Style.RESET_ALL}")
                        else:
                            print(f"{Fore.RED}âœ— Code coverage: {coverage}{Style.RESET_ALL}")
            else:
                print(f"{Fore.YELLOW}âš  Coverage not available{Style.RESET_ALL}")
        except Exception as e:
            print(f"{Fore.YELLOW}âš  Could not generate coverage report: {e}{Style.RESET_ALL}")
    
    def generate_test_report(self, results: Dict[str, Tuple[int, int]]):
        """Generate comprehensive test report"""
        self.print_section("TEST REPORT", "ðŸ“„")
        
        total_passed = sum(r[0] for r in results.values())
        total_failed = sum(r[1] for r in results.values())
        total_tests = total_passed + total_failed
        
        # Calculate success rate
        success_rate = (total_passed / total_tests * 100) if total_tests > 0 else 0
        
        # Print summary table
        print(f"\n{Fore.CYAN}Test Suite Results:{Style.RESET_ALL}")
        print(f"{Fore.CYAN}{'-'*50}{Style.RESET_ALL}")
        print(f"{'Suite':<20} {'Passed':<10} {'Failed':<10} {'Status':<10}")
        print(f"{'-'*50}")
        
        for suite, (passed, failed) in results.items():
            status = f"{Fore.GREEN}PASS{Style.RESET_ALL}" if failed == 0 else f"{Fore.RED}FAIL{Style.RESET_ALL}"
            print(f"{suite:<20} {passed:<10} {failed:<10} {status}")
        
        print(f"{'-'*50}")
        print(f"{'TOTAL':<20} {total_passed:<10} {total_failed:<10}")
        
        # Print summary
        print(f"\n{Fore.CYAN}Summary:{Style.RESET_ALL}")
        print(f"Total Tests: {total_tests}")
        print(f"Passed: {Fore.GREEN}{total_passed}{Style.RESET_ALL}")
        print(f"Failed: {Fore.RED}{total_failed}{Style.RESET_ALL}")
        print(f"Success Rate: {self._get_colored_percentage(success_rate)}%")
        
        # Save report to file
        report_path = self.output_dir / f"test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        report_data = {
            "timestamp": datetime.now().isoformat(),
            "duration_seconds": time.time() - self.start_time,
            "results": {k: {"passed": v[0], "failed": v[1]} for k, v in results.items()},
            "summary": {
                "total_tests": total_tests,
                "passed": total_passed,
                "failed": total_failed,
                "success_rate": success_rate
            }
        }
        
        with open(report_path, "w") as f:
            json.dump(report_data, f, indent=2)
        
        print(f"\n{Fore.CYAN}Report saved to: {report_path}{Style.RESET_ALL}")
        
        # Final status
        if total_failed == 0:
            print(f"\n{Back.GREEN}{Fore.WHITE} âœ… ALL TESTS PASSED! {Style.RESET_ALL}")
        else:
            print(f"\n{Back.RED}{Fore.WHITE} âŒ SOME TESTS FAILED {Style.RESET_ALL}")
        
        return total_failed == 0
    
    def _get_colored_percentage(self, percentage: float) -> str:
        """Get colored percentage based on value"""
        if percentage >= 90:
            return f"{Fore.GREEN}{percentage:.1f}{Style.RESET_ALL}"
        elif percentage >= 70:
            return f"{Fore.YELLOW}{percentage:.1f}{Style.RESET_ALL}"
        else:
            return f"{Fore.RED}{percentage:.1f}{Style.RESET_ALL}"
    
    def _create_unit_test_template(self, test_file: str):
        """Create unit test template"""
        Path(test_file).parent.mkdir(parents=True, exist_ok=True)
        
        template = '''"""Unit test template"""
import pytest
from certify_studio.core import *

class TestUnit:
    def test_example(self):
        assert True
'''
        
        with open(test_file, "w") as f:
            f.write(template)
    
    def _create_integration_test_template(self, test_file: str):
        """Create integration test template"""
        Path(test_file).parent.mkdir(parents=True, exist_ok=True)
        
        template = '''"""Integration test template"""
import pytest
import asyncio
from certify_studio.core import *

class TestIntegration:
    @pytest.mark.asyncio
    async def test_example(self):
        assert True
'''
        
        with open(test_file, "w") as f:
            f.write(template)
    
    def _create_performance_test(self):
        """Create performance test"""
        perf_dir = Path("tests/performance")
        perf_dir.mkdir(exist_ok=True)
        
        perf_test = '''"""Performance tests for Certify Studio"""
import pytest
import time
import asyncio
from httpx import AsyncClient

class TestPerformance:
    @pytest.mark.asyncio
    async def test_api_response_time(self):
        """Test API response times"""
        async with AsyncClient(base_url="http://localhost:8000") as client:
            times = []
            
            for _ in range(10):
                start = time.time()
                response = await client.get("/api/v1/info")
                elapsed = time.time() - start
                times.append(elapsed)
                
                assert response.status_code == 200
                assert elapsed < 1.0  # Should respond in under 1 second
            
            avg_time = sum(times) / len(times)
            assert avg_time < 0.5  # Average should be under 500ms
    
    @pytest.mark.asyncio
    async def test_concurrent_requests(self):
        """Test handling concurrent requests"""
        async with AsyncClient(base_url="http://localhost:8000") as client:
            # Send 50 concurrent requests
            tasks = []
            for _ in range(50):
                task = client.get("/health")
                tasks.append(task)
            
            start = time.time()
            responses = await asyncio.gather(*tasks)
            elapsed = time.time() - start
            
            # All should succeed
            assert all(r.status_code == 200 for r in responses)
            
            # Should complete within reasonable time
            assert elapsed < 5.0  # 50 requests in under 5 seconds
'''
        
        with open(perf_dir / "test_performance.py", "w") as f:
            f.write(perf_test)
        
        with open(perf_dir / "__init__.py", "w") as f:
            f.write("")
    
    def run_all_tests(self):
        """Run all test suites"""
        self.start_time = time.time()
        self.print_header()
        
        # Check if backend is running
        self.print_section("SYSTEM CHECK", "ðŸ”")
        try:
            import httpx
            response = httpx.get("http://localhost:8000/health")
            if response.status_code == 200:
                print(f"{Fore.GREEN}âœ“ Backend is running and healthy{Style.RESET_ALL}")
            else:
                print(f"{Fore.RED}âœ— Backend returned status {response.status_code}{Style.RESET_ALL}")
                return False
        except Exception as e:
            print(f"{Fore.RED}âœ— Backend is not running! Please start it first.{Style.RESET_ALL}")
            print(f"{Fore.YELLOW}Run: uv run uvicorn certify_studio.main:app --reload{Style.RESET_ALL}")
            return False
        
        # Run test suites
        results = {}
        
        # Unit tests
        results["Unit Tests"] = self.run_unit_tests()
        
        # Integration tests
        results["Integration Tests"] = self.run_integration_tests()
        
        # E2E tests
        results["E2E Tests"] = self.run_e2e_tests()
        
        # Performance tests
        results["Performance Tests"] = self.run_performance_tests()
        
        # Code coverage
        self.check_code_coverage()
        
        # Generate final report
        success = self.generate_test_report(results)
        
        # Print execution time
        duration = time.time() - self.start_time
        print(f"\n{Fore.CYAN}Total execution time: {duration:.2f} seconds{Style.RESET_ALL}")
        
        return success


def main():
    """Main entry point"""
    runner = TestRunner()
    success = runner.run_all_tests()
    
    # Exit with appropriate code
    sys.exit(0 if success else 1)


if __name__ == "__main__":
    main()
